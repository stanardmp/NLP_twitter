{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project description \n",
    "\n",
    "Competition Description\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n",
    "\n",
    "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n",
    "Acknowledgments\n",
    "\n",
    "This dataset was created by the company figure-eight and originally shared on their ‘Data For Everyone’ website here.\n",
    "\n",
    "Tweet source: https://twitter.com/AnyOtherAnnaK/status/629195955506708480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot  as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "#Read the data test ans train \n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "print(train.shape, test.shape) #, sub_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5) (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only the two columns relavant for the model here: target and text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 1) (7613, 2)\n"
     ]
    }
   ],
   "source": [
    "# # select only columns that are relevants for us ie target and test\n",
    "train = train[['text', 'target']]\n",
    "# # train.head()\n",
    "# # train = train.loc[:,['text', 'target']]\n",
    "\n",
    "# # # train = train.loc[:, ['text', 'target']]  # use only the two relevant columns\n",
    "\n",
    "# # test = test.loc[:, ['text']]\n",
    "test = test[['text']]\n",
    "# # test.head()\n",
    "print(test.shape, train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 Just happened a terrible car crash\n",
       "1  Heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           Apocalypse lighting. #Spokane #wildfires\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test[['text']]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test.isnull().sum()\n",
    "train.isnull().sum()\n",
    "\n",
    "# we have no null values on test and train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "\n",
    "* Lower case of the text\n",
    "* remmover Html\n",
    "* Remove special characters\n",
    "* Remover numbers\n",
    "* PLsit intor words (Tokenized the text)\n",
    "* Lematisation or stemmatisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Def_Clean_text_NLP.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Def_Clean_text_NLP.py\n",
    "\n",
    "# Load necessary libvraries\n",
    "import nltk \n",
    "import re \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "# write a function that clean the text \n",
    "def text_cleaning(text):\n",
    "#     text = str(text)\n",
    "    \n",
    "    # lower character\n",
    "    text1 = text.lower()\n",
    "    \n",
    "    # remmover Html\n",
    "    text2 = text1.replace('{html}', \" \")\n",
    "    \n",
    "    # Remove special characters\n",
    "    cleanr = re.compile('<,*?')\n",
    "    text3 = re.sub(cleanr, '', text2)\n",
    "    \n",
    "    # Remover numbers\n",
    "    text4 = re.sub('[0-9]+', '', text3)\n",
    "    \n",
    "    # tokenized the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens =  tokenizer.tokenize(text4)\n",
    "    \n",
    "    #  remove stop words from the tokenized text\n",
    "    filtered_words = [w for w in tokens if len(w)>2 if not w in stopwords.words('english')]\n",
    "    \n",
    "    # Lematisation \n",
    "    lemma_words = [WordNetLemmatizer().lemmatize(w) for w in filtered_words] \n",
    "    \n",
    "    # join all the words back into a cleaned text\n",
    "    join_words  = ' '.join(lemma_words)\n",
    "    \n",
    "    # return the cleaned text\n",
    "    return join_words\n",
    "    \n",
    "# ## aplly this to the data frame \n",
    "\n",
    "# data['cleaned_text'] = data['text'].map(lambda s: Preprocessing(s))\n",
    "# # pd.set_option('display.max_colwidth', None) #enable see full text in rows.\n",
    "\n",
    "# # # 3h. Print the first 5 rows of the datas after preprocessing.\n",
    "# data.head() \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 2) (7613, 2) (3263, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanardmp/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Combune train and test data to form totatal data to be cleaned \n",
    "\n",
    "# we can concatenate train and test\n",
    "\n",
    "frame = [train, test]\n",
    "data = pd.concat(frame, keys =['text', 'target'])\n",
    "\n",
    "print(data.shape, train.shape, test.shape)\n",
    "# = pd.concat(frames, keys=[\"x\", \"y\", \"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td rowspan=\"5\" valign=\"top\">text</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>forest fire near ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target                                               text  \\\n",
       "text 0     1.0  Our Deeds are the Reason of this #earthquake M...   \n",
       "     1     1.0             Forest fire near La Ronge Sask. Canada   \n",
       "     2     1.0  All residents asked to 'shelter in place' are ...   \n",
       "     3     1.0  13,000 people receive #wildfires evacuation or...   \n",
       "     4     1.0  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "                                             cleaned_text  \n",
       "text 0           deed reason earthquake may allah forgive  \n",
       "     1                 forest fire near ronge sask canada  \n",
       "     2  resident asked shelter place notified officer ...  \n",
       "     3  people receive wildfire evacuation order calif...  \n",
       "     4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now apply the clean text on our whole data using the prewriten function\n",
    "\n",
    "# write a function that clean the text \n",
    "def text_cleaning(text):\n",
    "#     text = str(text)\n",
    "    \n",
    "#     # lower character\n",
    "    text1 = text.lower()\n",
    "    \n",
    "#     # remmover Html\n",
    "    text2 = text1.replace('{html}', \" \")\n",
    "    \n",
    "#     # Remove special characters\n",
    "    cleanr = re.compile('<,*?')\n",
    "    text3 = re.sub(cleanr, '', text2)\n",
    "    \n",
    "#     # Remover numbers\n",
    "    text4 = re.sub('[0-9]+', '', text3)\n",
    "    \n",
    "#     # tokenized the text\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens =  tokenizer.tokenize(text4)\n",
    "    \n",
    "#     #  remove stop words from the tokenized text\n",
    "    filtered_words = [w for w in tokens if len(w)>2 if not w in stopwords.words('english')]\n",
    "    \n",
    "#     # Lematisation \n",
    "    lemma_words = [WordNetLemmatizer().lemmatize(w) for w in filtered_words] \n",
    "    \n",
    "#     # join all the words back into a cleaned text\n",
    "    join_words  = ' '.join(lemma_words)\n",
    "    \n",
    "    # return the cleaned text\n",
    "    return join_words\n",
    "    \n",
    "\n",
    "data['cleaned_text']= data['text'].map(lambda s : text_cleaning(str(s)))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0., nan])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the text into vectors to build the model with \n",
    "\n",
    "## Vectorization \n",
    "\n",
    "* Use here Back of words to convert each words into vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the appropriate library \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features= 1000)\n",
    "features = vectorizer.fit_transform(data['cleaned_text'])\n",
    "features = features.toarray()\n",
    "\n",
    "features  # for all data in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10876, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 1000)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split back the data to the trained and test data cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split back the data to the trained and test data cleaned\n",
    "\n",
    "d = data.drop('text', axis = 'columns')\n",
    "train_cleaned = d[:len(train)]\n",
    "# print(train.shape, train_cleaned.shape)\n",
    "test_cleaned = d[len(train):].drop('target', axis ='columns')\n",
    "# print(test.shape, test_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanardmp/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['train_cleaned.pkl']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can save the test_cleaned and train_cleaned data into pickle so that we do not have to clean up the data later on.\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(test_cleaned, 'test_cleaned.pkl')\n",
    "joblib.dump(train_cleaned, 'train_cleaned.pkl')\n",
    "\n",
    "\n",
    "# # load back the pickle data saved.\n",
    "# data = joblib.load('train_cleaned.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5329, 1000) (2284, 1000) (3263,)\n"
     ]
    }
   ],
   "source": [
    "# Split the training data to build the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # split data \n",
    "\n",
    "\n",
    "# convert train and test cleaned data into vectors first \n",
    "\n",
    "# train cleaned\n",
    "vectorizer = CountVectorizer(max_features= 1000)\n",
    "features_train = vectorizer.fit_transform(train_cleaned['cleaned_text'])\n",
    "x_train = features_train.toarray()\n",
    "\n",
    "\n",
    "# test cleaned \n",
    "features_test = vectorizer.fit_transform(test_cleaned['cleaned_text'])\n",
    "X_test = features_test.toarray()\n",
    "\n",
    "# let us assign the 30% of the data for validation\n",
    "# x_train = train_cleaned['cleaned_text']\n",
    "y_train = train_cleaned['target']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = .30, random_state=0)\n",
    "\n",
    "X_test = test_cleaned['cleaned_text']\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stanardmp/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test accuracy: 0.8695815349971853 and  Validation 0.781523642732049\n",
      "Random Forest Classifier Test accuracy: 0.9769187464815162 and  Validation 0.792031523642732\n",
      "Support Vector Machine Classifier Test accuracy: 0.9733533495965472 and  Validation 0.7727670753064798\n",
      "AdaBoost Classifier Test accuracy: 0.7969600300243949 and  Validation 0.7797723292469352\n",
      "Bagging Classifier Test accuracy: 0.9605929817977107 and  Validation 0.760507880910683\n",
      "Gradient boost Classifier Test accuracy: 0.7828860949521487 and  Validation 0.76138353765324\n",
      "KNeighbors Classifier Test accuracy: 0.8055920435353725 and  Validation 0.7377408056042032\n"
     ]
    }
   ],
   "source": [
    "### %%writefile Build_Pipeline.py\n",
    "\n",
    "# import the libraries usefull to build the classification models\n",
    "\n",
    "\n",
    "# to make the pipelines\n",
    "from sklearn.pipeline  import make_pipeline\n",
    "\n",
    "# classifications models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# cross validation to validate the model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# To scale the data before fiting the model\n",
    "from sklearn.preprocessing import MinMaxScaler # or scale bwte [0,1].. used in non statistical context\n",
    "from sklearn.preprocessing import StandardScaler # return z-score\n",
    "\n",
    "\n",
    "#  Build the pipeline for all the models in one\n",
    "\n",
    "pipe_rfc = make_pipeline(StandardScaler(),\n",
    "                        RandomForestClassifier(criterion='gini',\n",
    "                                               min_impurity_decrease= 0.0,\n",
    "                                               n_estimators = 100,\n",
    "                                               random_state = None\n",
    "                                              )\n",
    "                        )\n",
    "\n",
    "pipe_lr = make_pipeline(StandardScaler(),\n",
    "                       LogisticRegression(random_state = 2))\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(), \n",
    "                        SVC(kernel= 'rbf', C =30, gamma ='auto' )\n",
    "                        )\n",
    "\n",
    "pipe_ada = make_pipeline(StandardScaler(),\n",
    "                        AdaBoostClassifier(n_estimators=100, \n",
    "                                           random_state=0\n",
    "                                          ))\n",
    "\n",
    "pipe_bag = make_pipeline(StandardScaler(),\n",
    "                        BaggingClassifier())\n",
    "\n",
    "pipe_gboost = make_pipeline(StandardScaler(),\n",
    "                           GradientBoostingClassifier())\n",
    "\n",
    "pipe_knn = make_pipeline(StandardScaler(),\n",
    "                        KNeighborsClassifier())\n",
    "\n",
    "\n",
    "# All together in one pipeline\n",
    "pipelines = [pipe_lr, \n",
    "             pipe_rfc, \n",
    "             pipe_svc, \n",
    "             pipe_ada, \n",
    "             pipe_bag, \n",
    "             pipe_gboost, \n",
    "             pipe_knn]\n",
    "\n",
    "\n",
    "# Build a pipeline for each models here\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_classifier = 0\n",
    "best_pipeline = \"\"\n",
    "\n",
    "\n",
    "# Dictionary  of pipelines and clasfiers types  for ease  reference\n",
    "pip_dict ={0:'Logistic Regression', \n",
    "           1:'Random Forest Classifier',\n",
    "           2: 'Support Vector Machine Classifier',\n",
    "           3: 'AdaBoost Classifier',\n",
    "           4: 'Bagging Classifier',\n",
    "           5: 'Gradient boost Classifier',\n",
    "           6: 'KNeighbors Classifier'\n",
    "          }\n",
    "\n",
    "\n",
    "# Fit all the pipelines\n",
    "for pipe in  pipelines:\n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "# for all pipelines print the accuracy result to see which one has better performance\n",
    "\n",
    "for i , model in enumerate(pipelines):\n",
    "    print(\"{} Test accuracy: {} and  Validation {}\".format(pip_dict[i], model.score(X_train, Y_train), model.score(X_val, Y_val)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Random forest over fit the data,\n",
    "* Ada dont perform very well, but do not overfit the data\n",
    "* GBoost also dont perform very well, but do not overfit \n",
    "* \n",
    "\n",
    "* we can perform Gridseach hyper parameters for RF, SVM, to see if we can better our model without overfiting the data too m,uch.  If not work, try to improve the model that does not overgifit as Ada \n",
    "\n",
    "\n",
    "We can try to optimise the adaboost and/or try to perform ensemble techniques, like mixing models to optimize the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune the better model to improve the accuracy of the model\n",
    "#### we can use  Gridsearch \n",
    "\n",
    "\n",
    "* An important hyperparameter for AdaBoost algorithm is the number of decision trees used in the ensemble.\n",
    "\n",
    "* Recall that each decision tree used in the ensemble is designed to be a weak learner. That is, it has skill over random prediction, but is not highly skillful. As such, one-level decision trees are used, called decision stumps.\n",
    "\n",
    "* The number of trees added to the model must be high for the model to work well, often hundreds, if not thousands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model performance and choose the best model to be used.\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Initialise a dictionary with the best classification models here\n",
    "\n",
    "model_params = {\n",
    "    'svm' :{\n",
    "        'model': SVC(gamma ='auto'),\n",
    "        'params':{\n",
    "                  'C':[1,10,20],\n",
    "                   'kernel': ['rbf', 'linear']\n",
    "                 }\n",
    "           },\n",
    "            'random_forest':{\n",
    "            'model':RandomForestClassifier(),\n",
    "              'params':{\n",
    "            'n_estimators':[1,5,10]\n",
    "                        }\n",
    "           },\n",
    "           'logistic_regression':{\n",
    "            'model': LogisticRegression(solver ='liblinear',multi_class ='auto'),\n",
    "            'params':{\n",
    "                'C':[1,5,10]\n",
    "                     }\n",
    "     }               \n",
    "}\n",
    "\n",
    "\n",
    "scores = []\n",
    "for model_name, mp in model_params.items():\n",
    "    clf =  RandomizedSearchCV(mp['model'],\n",
    "                      mp['params'],\n",
    "                      cv = 5,  # cv = cross validation number\n",
    "                      return_train_score = False,\n",
    "                      n_iter =2)  # nur on RandomSearch.. Gridsearch takes too much time.\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    scores.append({\n",
    "                   'model': model_name,\n",
    "                    'best_score': clf.best_score_,\n",
    "                    'best_params': clf.best_params_\n",
    "                 })\n",
    "d = pd.DataFrame(scores ,columns =['model', 'best_score', 'best_params'])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERREEE STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'SAMME.R',\n",
       " 'base_estimator': None,\n",
       " 'learning_rate': 1.0,\n",
       " 'n_estimators': 50,\n",
       " 'random_state': None}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ada = AdaBoostClassifier()\n",
    "Ada.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFC = RandomForestClassifier()\n",
    "RFC.get_params()  # get all the hyper parameters for this model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'break_ties': False,\n",
       " 'cache_size': 200,\n",
       " 'class_weight': None,\n",
       " 'coef0': 0.0,\n",
       " 'decision_function_shape': 'ovr',\n",
       " 'degree': 3,\n",
       " 'gamma': 'scale',\n",
       " 'kernel': 'rbf',\n",
       " 'max_iter': -1,\n",
       " 'probability': False,\n",
       " 'random_state': None,\n",
       " 'shrinking': True,\n",
       " 'tol': 0.001,\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tune hyper parameters for the best performed model Svc \n",
    "\n",
    "svc = SVC()\n",
    "svc.get_params()  # get all the hyper parameters for this model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HERREEE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pipeline for this model with randomsearch to tweak the nmodel by selecting the better hyperparameter and improve the performance of the model.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# SVC\n",
    "svc_clf = GridSearchCV(SVC(gamma  = 'auto'),\n",
    "                       {'C': [1,10,20], \n",
    "                        'kernel': ['rbf', 'linear', 'poly']\n",
    "                       },\n",
    "                       cv =5,\n",
    "                       return_train_score=False)\n",
    "svc_clf.fit(X_train, Y_train)\n",
    "svc_clf.cv_result_   # print the cross validation result \n",
    "\n",
    "# export the result into pandas\n",
    "df_scv_tuning = pdf.DataFrame(svc_clf.cv_result_ )\n",
    "df_scv_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the result into pandas\n",
    "df_scv_tuning = pdf.DataFrame(svc_clf.cv_result_ )\n",
    "df_scv_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best cross-validation accuracy: 0.72\n",
      " Best parameters:  {'svc__kernel': 'rbf', 'svc__gamma': 1, 'svc__C': 10}\n",
      " Test set accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_grid_svc = {'svc__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "                  'svc__gamma': [0.001, 0.01, 0.1, 1], \n",
    "                  'svc__kernel':['rbf','poly']\n",
    "                 }\n",
    "\n",
    "pipe_svc = make_pipeline(MinMaxScaler(), (SVC())) \n",
    "\n",
    "## grid = GridSearchCV(pipe_svc , param_grid = param_grid_svc, cv = 5) \n",
    "\n",
    "samples = 10\n",
    "randomcv = RandomizedSearchCV(pipe_svc, param_distributions =param_grid_svc, n_iter=samples, cv =5)\n",
    "randomcv.fit( X_train, Y_train) \n",
    "\n",
    "print(\" Best cross-validation accuracy: {:.2f}\". format( randomcv.best_score_)) \n",
    "print(\" Best parameters: \", randomcv.best_params_) \n",
    "print(\" Test set accuracy: {:.2f}\". format( randomcv.score( X_val, Y_val)))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " le score a diminuer au lieu dimprouver?? whyyy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530647985989493"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random = randomcv.best_estimator_\n",
    "best_random.score(X_val , Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9748545693375867 0.7530647985989493\n"
     ]
    }
   ],
   "source": [
    "# use those hyperparameters to build the model\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "pipe_svc2 = make_pipeline(MinMaxScaler(), SVC(kernel= 'rbf', gamma= 1, C= 10))\n",
    "pipe_svc2.fit(X_train, Y_train)\n",
    "print(pipe_svc2.score(X_train, Y_train), pipe_svc2.score(X_val, Y_val))\n",
    "\n",
    "# ooopss we have humm like overfiting ... Noit good.. When we tune the model, it overfit?? grrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune another model, to try to improive the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "# randomForest clkadsifier tuning\n",
    "rfc = RandomForestClassifier() # Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "print(rfc.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best estimator: RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features=3,\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=6,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=50,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      " Best cross-validation accuracy: 0.78\n",
      " Best parameters:  {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 3, 'min_samples_leaf': 2, 'min_samples_split': 6}\n",
      " Test set accuracy train: 0.82\n",
      " Test set accuracy val: 0.80\n"
     ]
    }
   ],
   "source": [
    "# define grid of hyper parameters\n",
    "# specify parameters and distributions to sample from\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "rfcl = RandomForestClassifier(n_estimators=50)\n",
    "param_rfcl = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "samples = 10  # number of random samples \n",
    "randomCV = RandomizedSearchCV(rfcl, param_distributions=param_rfcl, n_iter=samples, cv =5) #default cv = 3\n",
    "\n",
    "randomCV.fit(X_train, Y_train)\n",
    "\n",
    "# evaluer le model\n",
    "print(\" Best estimator:\", randomCV.best_estimator_)\n",
    "print(\" Best cross-validation accuracy: {:.2f}\". format( randomCV.best_score_)) \n",
    "print(\" Best parameters: \", randomCV.best_params_) \n",
    "print(\" Test set accuracy train: {:.2f}\". format(randomCV.score( X_train, Y_train)))\n",
    "print(\" Test set accuracy val: {:.2f}\". format(randomCV.score( X_val, Y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                                        class_weight=None, criterion='gini',\n",
      "                                        max_depth=None, max_features=3,\n",
      "                                        max_leaf_nodes=None, max_samples=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=2, min_samples_split=6,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=100, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False) Test accuracy: 0.8192906736723587 0.7972854640980735\n"
     ]
    }
   ],
   "source": [
    "# Ok, now we dont see an overfit ..and the performance has improved a little bit\n",
    "# use those best parameter to build the final model r\n",
    "\n",
    "pipe_RF = make_pipeline(MinMaxScaler(), RandomForestClassifier(n_estimators=100,\n",
    "                                                                criterion ='gini', \n",
    "                                                               max_depth = None, \n",
    "                                                               max_features = 3,\n",
    "                                                              min_samples_leaf = 2, \n",
    "                                                               min_samples_split = 6)\n",
    "                       )\n",
    "\n",
    "# fit the pipeline\n",
    "pipe_RF.fit(X_train, Y_train)\n",
    "\n",
    "# evaluer le model\n",
    "print(\"{} Test accuracy: {} {}\".format(pipe_RF, pipe_RF.score(X_train, Y_train), pipe_RF.score(X_val, Y_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Deploy this and see the result\n",
    "\n",
    "# y_pred = pipe_RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.floor(np.expm1(pipe_RF.predict(x_test)))  # np.expm1 to have better precisiion in the number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = [int(i) for i in y_predict]  # convert all into int type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = y_predict\n",
    "sub.to_csv('submission1.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This submission gives me a score of  0.78 in Kaggle , Hummm, Cest pas bon du tout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hin:  tune all the models and combine them to have a strong predictor model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERRRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 1 model to submit with\n",
    "model =   # for exple\n",
    "y_predict = np.floor(np.expm1(model.predict(x_test)))  # np.expm1 to have better precisiion in the number.\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_id\n",
    "sub['target'] = y_predict\n",
    "sub.to_csv('submission_'+str(model)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dea\\l with missing values, look at skewness,\n",
    "#  Now slit not  with the function split but simply in the same order you combiuned them\n",
    "# Slit into train and test data set \n",
    "\n",
    "\n",
    "# A la fin save in two columns \n",
    "# y_predict = np.floor(np.expm1(xgb_model.predict(x_test)))\n",
    "\n",
    "# sub = pd.DataFrame()\n",
    "# sub['Id'] = test_id\n",
    "# sub['SalePrice'] = y_predict\n",
    "# sub.to_csv('submission2_xgboost.csv',index=False)\n",
    "\n",
    "# ou bien ceci\n",
    "# result = model.predict(test_data_features)\n",
    "\n",
    "#  output = pd.DataFrame(data ={\"id\":test[\"id\"], \"sentiment\": result})\n",
    "#  write teh file\n",
    "#  output.to_csv(\"name_submission_file.csv\", index =False, quoting =3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A lot of missing values \n",
    " \n",
    " visualize missing values using ggplot2\n",
    " \n",
    " https://www.kaggle.com/elenapetrova/joyful-analysis-of-death-causes/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
